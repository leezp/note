## 子域名清洗工具

用于处理 解析域名时出现的上万个子域名，进行去重

测试环境：windows 2cpu  url个数：73000

消耗时间: 20min/70000站  很多站都是pending超时，达到最大超时时间5s, 平均每分钟3500站，会持续优化

手动去重 73000约需2.5h ,经过对比与手动去重结果基本一致

添加代理访问效果更佳

**注意**

确保网络带宽充足，查看是否被路由限速，默认500并发，如果被限速，可能需要对应降低并发数，否则会导致异步爬虫获取数据不全使xpath匹配不到，title 仍可匹配到

页面是异步加载返回的，它不是直接返回请求的数据，高并发异步爬虫有时只能抓取到最开始的部分数据，后面的数据是由Javascript异步加载进来的，你也可以改成用python的selenium库，用webdriver加载页面来获取内容，然后再用xpath解析里面的数据


<br/>

**关于效率**

asyncio内部用到了select，而select就是系统打开文件数是有限度的，这个其实是操作系统的限制，linux打开文件的最大数默认是1024，windows默认是509，超过了这个值，程序就开始报错

可以限制并发数，也可以修改操作系统打开文件数的最大限制，在系统里有个配置文件可以修改默认值

<br/>

**Todo**：

1. 调研 group(1) 在高并发情况下是否获取不到完整title，导致匹配不上  比如大麦网  的title "缅怀XXX-大麦网404"

2. 考虑把 淘宝类字典归一

<br/>


**历史版本**

* 2020-01-03|<v0.02>|

1.降低程序耦合，改为命令行输入

	usage='python36 %(prog)s -f targetUrl_full.txt -b False/True'

只需修改 dic_key(True|False),input_file  # dic 字典里 含有域名时为True，否则为False

dic 按需修改 ,元组表示需要同时满足2个条件，每个列表元祖个数上限为1


* 2019-12-31|<v0.01>|

1.多进程协程对子域名清洗

  清洗逻辑: 状态码,title,网页内容匹配xpath


      
   
  